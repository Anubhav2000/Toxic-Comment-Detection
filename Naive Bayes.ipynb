{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Dropout, Embedding, Activation, Reshape, RepeatVector, LSTM, Bidirectional\n",
    "from keras.models import Model, Sequential, load_model, save_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv', header=0, sep=',', quotechar='\"', usecols=['target', 'comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv',)\n",
    "\n",
    "comments = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding stopwords ...\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "new_stop_words=set(stop_words)\n",
    "\n",
    "# adding woudlnt type of words into stopwords list\n",
    "for s in stop_words:\n",
    "\tnew_stop_words.add(s.replace('\\'',''))\n",
    "\tpass\n",
    "\t\n",
    "stop_words=new_stop_words\n",
    "print(\"Excluding stopwords ...\")\n",
    "\n",
    "# removing @ from default base filter, to remove that whole word, which might be considered as user or page name\n",
    "base_filters='\\n\\t!\"#$%&()*+,-./:;<=>?[\\]^_`{|}~ '\n",
    "\n",
    "# comments = pd.concat([train['comment_text'], train['target']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def removeStopWords(comment):\n",
    "   return ' '.join([word.lower() for word in comment.split() if word not in stop_words])\n",
    "#   row['comment_text'] = ' '.join([lemmatizer.lemmatize(word) for word in row['comment_text'].split()])\n",
    "#   print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_mapping = {\"_\":\" \", \"`\":\" \"}\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, ' ')     \n",
    "    return text\n",
    "# test['comment_text'] = test['comment_text']\n",
    "comments['comment_text'] = comments['comment_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "comments['comment_text'] = comments['comment_text'].apply(removeStopWords)\n",
    "# test['comment_text'] = test['comment_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['comment_text'] = test['comment_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "test['comment_text'] = test['comment_text'].apply(removeStopWords)\n",
    "# test.to_csv('test_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNanNull(text):\n",
    "  if text==np.nan:\n",
    "    text=''\n",
    "  return text\n",
    "# print(replaceNanNull(np.nan))\n",
    "comments['comment_text'] = comments['comment_text'].replace(np.nan, '')\n",
    "test['comment_text'] = test['comment_text'].replace(np.nan, '')\n",
    "comments['comment_text'].dropna(inplace=True)\n",
    "test['comment_text'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1804874 entries, 0 to 1804873\n",
      "Data columns (total 45 columns):\n",
      "id                                     int64\n",
      "target                                 float64\n",
      "comment_text                           object\n",
      "severe_toxicity                        float64\n",
      "obscene                                float64\n",
      "identity_attack                        float64\n",
      "insult                                 float64\n",
      "threat                                 float64\n",
      "asian                                  float64\n",
      "atheist                                float64\n",
      "bisexual                               float64\n",
      "black                                  float64\n",
      "buddhist                               float64\n",
      "christian                              float64\n",
      "female                                 float64\n",
      "heterosexual                           float64\n",
      "hindu                                  float64\n",
      "homosexual_gay_or_lesbian              float64\n",
      "intellectual_or_learning_disability    float64\n",
      "jewish                                 float64\n",
      "latino                                 float64\n",
      "male                                   float64\n",
      "muslim                                 float64\n",
      "other_disability                       float64\n",
      "other_gender                           float64\n",
      "other_race_or_ethnicity                float64\n",
      "other_religion                         float64\n",
      "other_sexual_orientation               float64\n",
      "physical_disability                    float64\n",
      "psychiatric_or_mental_illness          float64\n",
      "transgender                            float64\n",
      "white                                  float64\n",
      "created_date                           object\n",
      "publication_id                         int64\n",
      "parent_id                              float64\n",
      "article_id                             int64\n",
      "rating                                 object\n",
      "funny                                  int64\n",
      "wow                                    int64\n",
      "sad                                    int64\n",
      "likes                                  int64\n",
      "disagree                               int64\n",
      "sexual_explicit                        float64\n",
      "identity_annotator_count               int64\n",
      "toxicity_annotator_count               int64\n",
      "dtypes: float64(32), int64(10), object(3)\n",
      "memory usage: 619.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "# comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.TreebankWordTokenizer()\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "  return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "comments['comment_text'] = comments['comment_text'].apply(lemmatize)\n",
    "test['comment_text'] = test['comment_text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToClassification(target):\n",
    "  if target>=0.5:\n",
    "    target = 1\n",
    "  else:\n",
    "    target = 0\n",
    "  return target\n",
    "comments['target'] = comments['target'].apply(convertToClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "max_df = 0.3\n",
    "min_df = 0.003\n",
    "texts = comments[comments['target']>=0.2]['comment_text']\n",
    "tfidf = TfidfVectorizer(min_df = np.int(min_df*texts.shape[0]), max_df = max_df, ngram_range = (1,3))\n",
    "features = tfidf.fit_transform(texts)\n",
    "vectorizer = pd.DataFrame(features.todense(), columns=tfidf.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.transform(comments['comment_text'])\n",
    "Y = comments['target']\n",
    "x_train = X\n",
    "y_train = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import average_precision_score, roc_curve, roc_auc_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "logisticRegr = LogisticRegression(C=2)\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "# predictions1 = logisticRegr.predict_proba(x_test)\n",
    "# predictions1 = pd.DataFrame(predictions1)\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train, y_train)\n",
    "# predictions = classifier.predict_proba(x_test)\n",
    "# predictions = pd.DataFrame(predictions)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import svm\n",
    "# classifier = svm.LinearSVC(dual=False, C = 0.5)\n",
    "# classifier.fit(x_train, y_train)\n",
    "# predictions = classifier.predict(x_test)\n",
    "# # predictions = pd.DataFrame(predictions)\n",
    "\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "# auc = roc_auc_score(y_test, predictions)\n",
    "# print('AUC: %.3f' % auc)\n",
    "# # plot no skill\n",
    "# pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "# # plot the roc curve for the model\n",
    "# pyplot.plot(fpr, tpr, marker='.')\n",
    "# # show the plot\n",
    "# pyplot.show()\n",
    "# classification_report(y_test, predictions>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.drop(['Unnamed: 0'], axis=1)\n",
    "# test['comment_text'] = test['comment_text'].replace(np.nan, '')\n",
    "# test['comment_text'].dropna(inplace=True)\n",
    "# test['comment_text'] = subData['comment_text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = tfidf.transform(test['comment_text'])\n",
    "logProba = logisticRegr.predict_proba(final)\n",
    "NBProba = classifier.predict_proba(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1\n",
      "0      0.043898\n",
      "1      0.014190\n",
      "2      0.030241\n",
      "3      0.013952\n",
      "4      0.977324\n",
      "5      0.014774\n",
      "6      0.020411\n",
      "7      0.022490\n",
      "8      0.008589\n",
      "9      0.020175\n",
      "10     0.032462\n",
      "11     0.045941\n",
      "12     0.033306\n",
      "13     0.012297\n",
      "14     0.009216\n",
      "15     0.006408\n",
      "16     0.144596\n",
      "17     0.020058\n",
      "18     0.165747\n",
      "19     0.064921\n",
      "20     0.021583\n",
      "21     0.017588\n",
      "22     0.011791\n",
      "23     0.298244\n",
      "24     0.815754\n",
      "25     0.018712\n",
      "26     0.013827\n",
      "27     0.010451\n",
      "28     0.026662\n",
      "29     0.033588\n",
      "...         ...\n",
      "97290  0.019368\n",
      "97291  0.752601\n",
      "97292  0.076200\n",
      "97293  0.005639\n",
      "97294  0.023220\n",
      "97295  0.016306\n",
      "97296  0.014030\n",
      "97297  0.011967\n",
      "97298  0.013599\n",
      "97299  0.023433\n",
      "97300  0.011985\n",
      "97301  0.014870\n",
      "97302  0.014677\n",
      "97303  0.026359\n",
      "97304  0.021583\n",
      "97305  0.070265\n",
      "97306  0.019214\n",
      "97307  0.009683\n",
      "97308  0.023641\n",
      "97309  0.139521\n",
      "97310  0.061409\n",
      "97311  0.077650\n",
      "97312  0.082159\n",
      "97313  0.201343\n",
      "97314  0.022589\n",
      "97315  0.022022\n",
      "97316  0.012917\n",
      "97317  0.030346\n",
      "97318  0.179470\n",
      "97319  0.013970\n",
      "\n",
      "[97320 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "logProba = pd.DataFrame(logProba)\n",
    "NBProba = pd.DataFrame(NBProba)\n",
    "logProba.rename(index=str, columns={'1':'prediction'})\n",
    "del NBProba[0]\n",
    "del logProba[0]\n",
    "NBProba.rename(index=str, columns={'1':'prediction'})\n",
    "print(logProba)\n",
    "logProba = logProba.set_index([test['id']])\n",
    "NBProba = NBProba.set_index([test['id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb  __output__.json\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBProba['prediction'] = NBProba[1]\n",
    "logProba['prediction'] = logProba[1]\n",
    "NBProba = NBProba.drop(1, axis=1)\n",
    "logProba = logProba.drop(1, axis=1)\n",
    "logProba = logProba.set_index([test['id']])\n",
    "NBProba = NBProba.set_index([test['id']])\n",
    "# NBProba.to_csv('nbproba.csv')\n",
    "# logProba.to_csv('logProba.csv')\n",
    "# os.chdir('..')\n",
    "NBProba.to_csv('submission.csv')\n",
    "# logProba.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = pd.read_csv('../input/sample_submission.csv')\n",
    "# sample.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
